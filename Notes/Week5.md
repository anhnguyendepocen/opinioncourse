# Week 5 Notes #
October 2, 2013

## General questions ##
* Did you learn anything from this week's readings? If so, what?
* What aspect(s) of this week's readings did you find the most compelling?
* What aspect(s) of this week's readings did you find the most difficult or challenging?


# Readings #

Note change to syllabus: Please read Roger Tourangeau and Kenneth A. Rasinski. Cognitive Processes Underlying Context
Effects in Attitude Measurement. *Psychological Bulletin*, 103(3):299â€“314, 1988.

## Tourangeau and Rasinski ##
* What are the four steps that Tourangeau and Rasinski say are involved in answering opinion questions on surveys? (299--300)
 * Does their theory assume attitudes are memory-based, online, or doesn't it matter? Why?
* Tourangeau and Rasinski discuss the difference between a stable attitude and a stable response (301). What are the implications of this distinction for interpreting public opinion?
* The article focuses on two types of "context effects". What are they? (301)
* How do those effects manifest at each of the four stages of the response process?
* When survey respondents are asked to express opinions, Tourangeau and Rasinski argue that they have to map their response (the opinion) onto the set of alternatives available to them in a given opinion question. Does this mean that opinion questions (in open or closed ended formats) are a valid measure of opinions?
* What is "editing" and why do respondents do it? (307--308) Does it provide more or less meaningful (or accurate) responses? Should people edit?
* What are the factors that affect context effects at each stage of the question-answering process? (Table 3)
* Can we trust the responses people give to surveys as accurate measures of their opinions (and thus of public opinion in general)?
 * Thinking back to Herbst, how do the context effects discussed by Tourangeau and Rasinski enable "public opinion" to be used as a symbol in political debate?


## Bishop, Tuchfarber, and Oldendick ##
* Why should we care about the research being reported by Bishop et al? Does it matter? If so, how? If no, why not?
* What are the three different experimental conditions used by Bishop et al.? (243)
* Does their decision create conditions that you would expect *a priori* to produce lots of "don't know" responses or few? Why?
* Why do they argue that people might provide opinions to ficitious issue questions?
* What are their findings? How often do people report opinions on fictious issues?
* What implications do their findings have for interpreting public opinion data from surveys?
* What implications do their findings have for designing and implementing survey-based opinion research?
* Do any ethical considerations come to mind regarding this study? And do Bishop et al. address those considerations?


## Schuldt, Konrath, and Schwarz ##

* What do Schuldt et al. aim to do? Are they trying to understand if people distinguish "climate change" from "global warming" (i.e., that they have separate opinions about each) or that the opinions people *report* depend on how the question is asked? How well do they test that?
* What do they find? Does question wording matter? If so, how? And for whom?
* What implications does this study (and studies like it) have for interpreting opinion data from survey?
* What implications does it have for the question of whether citizens are rational or reasoned?
