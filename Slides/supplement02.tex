\input{preamble}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{decorations.pathreplacing}

\title{Methods Supplementary Lecture 2:\\More Survey Design and Analysis}

\date[]{}

\begin{document}

\frame{\titlepage}

\frame{\tableofcontents}


\section{Basic Quantitative Analysis}
\frame{\tableofcontents[currentsection]}


\subsection{Opinion Questions}
\frame{\tableofcontents[currentsection]}


\frame{
	\frametitle{Evaluative questions}
	\begin{itemize}\itemsep1em
		\item Name an object of evaluation
		\item Possibly describe that object
		\item Ask for a transformation of the evaluation onto a set of responses
	\end{itemize}
}

\frame{
	\frametitle{Question templates}

	\small
	
	\begin{itemize}
		\item Ratings
			\begin{itemize}
				\item Several varieties of rating scales
			\end{itemize}
		\item Scales/Thermometers
		\item Agree-disagree
		\item Forced choices
		\item Open-ended
		\item Rankings (note: need alternatives to rank against)
	\end{itemize}
}

\frame{
	\frametitle{Extended Example}
	\begin{itemize}\itemsep1em
		\item Public opinion survey in Great Britain
		\item Construct: Opinion toward UK involvement in air strikes on Islamic State militants in Iraq and Syria
		\item Think about strengths and weaknesses of each question
	\end{itemize}
}

\frame{
	\frametitle{{\large Example: Rating (bipolar)}}
	Do you support or oppose Great Britain's participation in U.S.-led air strikes on Islamic State (IS) in Iraq and Syria?
	\begin{itemize}
		\item Strongly support
		\item Somewhat support
		\item Neither support nor oppose
		\item Somewhat oppose
		\item Strongly oppose
	\end{itemize}
}

\frame{
	\frametitle{{\large Example: Rating (branching)}}
	
	\footnotesize
	
	Do you support or oppose Great Britain's participation in U.S.-led air strikes on Islamic State (IS) in Iraq and Syria?
	\begin{itemize}
		\item Support
		\item Neither support nor oppose
		\item Oppose
	\end{itemize}
	\vspace{1em}
	Would you say that you strongly [support|oppose] or somewhat [support|oppose] Great Britain's participation?
	\begin{itemize}
		\item Strongly
		\item Somewhat
	\end{itemize}
}

\frame{
	\frametitle{{\large Example: Rating (bipolar)}}
	Are you favourable or unfavourable toward Great Britain's participation in U.S.-led air strikes on Islamic State (IS) in Iraq and Syria?
	\begin{itemize}
		\item Very favourable
		\item Somewhat favourable
		\item Neither favourable nor unfavourable
		\item Somewhat unfavourable
		\item Strongly unfavourable
	\end{itemize}
}

\frame{
	\frametitle{{\large Example: Rating (unipolar)}}
	To what extent do you support Great Britain's participation in U.S.-led air strikes on Islamic State (IS) in Iraq and Syria?
	\begin{itemize}
		\item Strongly
		\item Moderately
		\item Somewhat
		\item Not at all
	\end{itemize}
}

\frame{
	\frametitle{{\large Example: Rating (unipolar)}}
	How favourable are you toward Great Britain's participation in U.S.-led air strikes on Islamic State (IS) in Iraq and Syria?
	\begin{itemize}
		\item Extremely favourable
		\item Very favourable
		\item Moderately favourable
		\item Somewhat favourable
		\item Not at all favourable
	\end{itemize}
}

\frame{
	\frametitle{{\large Example: Numbered Scale}}
	
	\small
	
	On a scale from 1 to 5, with 1 being ``strongly oppose'' and 5 being ``strongly support,'' to what extent do you support Great Britain's participation in U.S.-led air strikes on Islamic State (IS) in Iraq and Syria?
	\begin{enumerate}
		\item Strongly oppose
		\item 
		\item 
		\item 
		\item Strongly support
	\end{enumerate}
}

\frame{
	\frametitle{{\large Example: Thermometer}}
	
	\footnotesize
	
	We would like to get your feelings toward some of political policies. Please rate your support for the policy using something we call the feeling thermometer. Ratings between 50 degrees and 100 degrees mean that you feel favourable and warm toward the policy. Ratings between 0 degrees and 50 degrees mean that you don't feel favourable toward the policy. You would rate the policy at the 50 degree mark if you don't feel particularly favourable or unfavourable toward.
	
	\vspace{1em}
	Great Britain's participation in U.S.-led air strikes on Islamic State (IS) in Iraq and Syria.
	\begin{itemize}
		\item 0--100 slider
	\end{itemize}
}


\frame{
	\frametitle{{\normalsize Example: Agree/Disagree (bipolar)}}
	To what extent do you agree with the following statement: I support Great Britain's participation in U.S.-led air strikes on Islamic State (IS) in Iraq and Syria.
	\begin{itemize}
		\item Strongly agree
		\item Somewhat agree
		\item Neither agree nor disagree
		\item Somewhat disagree
		\item Strongly disagree
	\end{itemize}
}

\frame{
	\frametitle{{\normalsize Example: Agree/Disagree (unipolar)}}
	To what extent do you agree with the following statement: I support Great Britain's participation in U.S.-led air strikes on Islamic State (IS) in Iraq and Syria.
	\begin{itemize}
		\item Agree completely
		\item Agree to a large extent
		\item Agree to a moderate extent
		\item Agree a little bit
		\item Agree not at all
	\end{itemize}
}

\frame{
	\frametitle{{\large Example: Forced choice}}
	When thinking about Great Britain's participation in U.S.-led air strikes on Islamic State (IS) in Iraq and Syria, which of the following comes closer to your opinion:
	\begin{itemize}
		\item Great Britain should participate in air strikes
		\item Great Britain should not participate in air strikes
	\end{itemize}
}

\frame{
	\frametitle{{\large Example: Open-ended}}
	In your own words, how would you describe your opinion on Great Britain's participation in U.S.-led air strikes on Islamic State (IS) in Iraq and Syria?
}


\frame{
	\frametitle{Additional Considerations}
	\begin{itemize}\itemsep0.5em
		\item How many response categories?
		\item Middle category (presence and label)
		\item ``no opinion'' and/or ``don't know'' options
		\item Probe if ``no opinion'' or ``don't know''?
		    \begin{itemize}
                \item Encourage guessing?
    		    \item Clarify/describe object of evaluation?
		    \end{itemize}
		\item Branching format?
		\item Order of response categories
		\item Changes based on survey mode
	\end{itemize}
}


\subsection{Analyzing Opinion Measures}
\frame{\tableofcontents[currentsection]}

\frame{

\frametitle{Common Statistics in Opinion Research}

\begin{itemize}\itemsep1em
\item Counts (tabulation)
\item Means
\item Proportions
\item Correlations
\item Differences
\item Regression coefficients 
\end{itemize}

}


\frame{

	\frametitle{Tabulation}
	
	\small
	
	\begin{itemize}\itemsep0.5em
	\item Tabulation is simply the counting of numbers of observed values of a given variable, or the combination of values across multiple variables
	\item In practice, we almost always translate these to proportions
		\begin{itemize}
		\item For rare subgroups, we might be interested in counts (e.g., number of drug users, etc.)
		\end{itemize}
	\item<2-> In complex surveys, these tabulations have to be weighted
		\begin{itemize}
		\item In practice, almost always done using specialized software
		\end{itemize}
	\end{itemize}

}


\frame{
	\frametitle{Sample mean}
	\begin{equation}
	\bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_i
	\end{equation}
	where $y_i = $ value for a unit, and\\
	$n = $ sample size
	
	\begin{equation}
	SE_{\bar{y}} = \sqrt{(1-f)\frac{s^2}{n}}
	\end{equation}
	where $f = $ proportion of population sampled,\\
	$s^2 = $ sample (element) variance, and\\
	$n = $ sample size
}

\frame{
	\frametitle{Sample proportion}
	\begin{equation}
	\bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_i
	\end{equation}
	where $y_i = $ value for a unit, and\\
	$n = $ sample size
	
	\begin{equation}
	SE_{\bar{y}} = \sqrt{\frac{(1-f)}{(n-1)}p(1-p)}
	\end{equation}
	where $f = $ proportion of population sampled,\\
	$p = $ sample proportion, and\\
	$n = $ sample size
}


\frame{

	\frametitle{Making comparisons}
	\begin{itemize}\itemsep1em
	\item Differences of means
	\item Differences of proportions
	\item Correlation between variables
	\item Regression estimates
	\end{itemize}
}


\frame{
	\frametitle{Mean Difference}
	\begin{itemize}\itemsep1em
	\item We might expect two means to differ
	    \begin{itemize}
		\item Means for two subgroups (e.g., mean and women)
		\item Means across two areas (e.g., England and Scotland)
		\item Means at two times (e.g., pre- and post-election)
		\end{itemize}
	\item We calculate this as a simple difference of subgroup means:\\
		$\beta = \dfrac{\sum_{i=1}^{n_1} x_{i1}}{n_1} - \dfrac{\sum_{i=1}^{n_0} x_{i0}}{n_0} $
	\end{itemize}

}


\frame{

\begin{center}
\includegraphics[height=\textheight]{images/meandifference}
\end{center}
}


\frame{
	\frametitle{Significance}
	
	\begin{itemize}\itemsep1em
	\item Is this difference large?
	\item We can speak about that in two ways:
		\begin{itemize}
		\item Is the difference substantively large?
		\item Is the difference larger than we would expect?
		\end{itemize}
	\item<2-> To answer the second question requires two pieces of information:
		\begin{itemize}
		\item A \textit{null} expectation for the difference, $H_0$
		\item An understanding of how likely we are to see a difference this large given that null expectation
		\end{itemize}
	\end{itemize}

}

\frame{
	\frametitle{$t$-statistic}
	
	\small
	
	\begin{enumerate}\itemsep0.5em
	\item State a null expectation
		\begin{itemize}
		\item ``No difference'' null: $t_{\hat{\beta_1}} = \frac{\hat{\beta_1}}{SE_{\hat{\beta_1}}}$
		\item Any other null: $\frac{\hat{\beta_1} - H_0}{SE_{\hat{\beta_1}}}$, where $H_0$ is our null hypothesis value
		\end{itemize}
	\item Assessment of variability under null comes from SE
		\begin{itemize}
		\item Inherent (element) variance of our data
		\item Sample size
		\item Sampling procedures
		\end{itemize}
	\item Effect is ``statistically significant'' if the probability of seeing a $t$-statistic this large or larger is small
	\end{enumerate}
}

\frame{

\begin{center}
\includegraphics[height=\textheight]{images/twotailed}
\end{center}
}

\frame{

\begin{center}
\includegraphics[height=\textheight]{images/onetailed}
\end{center}

}


\frame{

We consider a difference ``statistically significant'' when it differs more from our null expectation than the variation in sample statistics we would expect to observe across repeated samples from a population where our null hypothesis was true (in this case, where there was actually no difference between the groups).

}

% In other words, the sample statistic is so unusual for a the population from which we --- under the null hypothesis --- believe that the data are drawn from, that we decide that the data are actually drawn from a population with a different population mean. That's the essence of statistical hypothesis testing.


\frame{

	\frametitle{Common Misconceptions}
	
	The $p$-value is not:
	
	\begin{itemize}
	\item The probability that a hypothesis is true or false
	\item A reflection of our confidence or certainty about the result
	\item The probability that the true mean is in any particular range of values
	\item A statement about the importance or substantive size of the effect
	\end{itemize}

}



\frame{}


\frame{

\frametitle{{\large Assessing Measurement Quality}}

\begin{enumerate}\itemsep1em
\item Conceptual clarity
\item Construct validity
	\begin{itemize}
	\item Convergent validity
	\item Divergent validity
	\end{itemize}
\item Accuracy and precision
\end{enumerate}

}

\frame{
\frametitle{Assessing Measures I}

\begin{itemize}\itemsep1em
\item Conceptual clarity is about knowing what we want to measure
\item Sloppy concepts make for bad measures
	\begin{itemize}
	\item Ambiguity % multiple meanings or multiple labels
	\item Vagueness % concept without a definition
	\end{itemize}
\item<2-> Revise concept definition as needed
\end{itemize}
}


\frame{
\frametitle{Assessing Measures II}

\begin{itemize}\itemsep0.5em
\item Construct validity is the degree to which a variable measures a concept
\item<2-> Construct validity is \textbf{high} if a variable is a measure of the concept we care about
\item<3-> Construct validity is \textbf{low} if a variable is actually a measure of something else
\end{itemize}
}

% sources: bad concept definition; totally inappropriate measures (using income to measure weight); measure becomes the concept (actual income is replaced by self-reported income)

\frame{

\frametitle{{\large Assessing Construct Validity}}

\begin{itemize}\itemsep1em
\item Multiple measures!
\item Look for:
	\begin{itemize}
	\item Convergence (Convergent validity)
	\item Discrimination (Discriminant validity)
	\end{itemize}
\item<2-> For example, the multi-trait, multi-method matrix
\end{itemize}
}


\frame{
\frametitle{Accuracy and Precision}
{\small Synonyms: accurate, true, correct, unbiased, valid}\\
{\small Synonyms: precise, certain, exact, specific, low variance}\\

\begin{center}
\includegraphics[width=0.5\textwidth]{images/accuracyprecision}
\end{center}

\vspace{-1em}
{\tiny \href{https://commons.wikimedia.org/wiki/File:Reliability_and_validity.svg}{Image Source: Wikimedia}, Nevit Dilmen}
}



\frame{
	\frametitle{Scaling}
	\begin{itemize}\itemsep0.5em
	\item Improve precision (and accuracy) by combining multiple noisy measures
	\item Common methods
		\begin{itemize}
		\item Simple sum or average
		\item Factor analysis
		\item Item response theory models
		\end{itemize}
	\item In survey context, typically use prepared, time-optimized batteries of items
	\end{itemize}
}


\frame{
	\frametitle{Cronbach's $\alpha$}
	\begin{itemize}\itemsep1em
	\item Scaling only makes sense if variables ``go together''
		\begin{itemize}
		\item We can assess them \textit{pairwise} by looking at correlations between variables
		\item But it's helpful to have a way to assess the scale as a whole
		\end{itemize}
	\item Definition:
		$\alpha = \dfrac{ N \bar{c} }{ \bar{v} + (N-1) \bar{c} }$
		\begin{itemize}
		\item N: number of items
		\item $\bar{c}$: average covariance of items
		\item $\bar{v}$: average variance of items
		\end{itemize}
	\end{itemize}
}



\section{Hierarchical Data Collection}
\frame{\tableofcontents[currentsection]}

\frame{

\frametitle{Hierarchical Data Collection}

\begin{itemize}\itemsep1em
\item Sampling is just a building block
\item Various elements can be combined:
	\begin{itemize}
	\item Repeated, independent sampling
	\item Repeated interviewing
	\item Parallel sampling w/in different contexts
	\item Experimentation (see Lecture 3)
	\end{itemize}
\item<2-> Big question: How do analyze complex data structures?
\end{itemize}

}

\frame{
	\frametitle{Research Questions}
	
	\small
	
	\begin{itemize}\itemsep1em
	\item Sampling allows us to make claims about populations
	\item Hierarchical data collection allows us to answer more complex research questions
		\begin{itemize}
		\item How does a population change?
		\item How do populations compare (across space and time)?
		\item How do individuals change?
		\item How are multiple factors related? How do those relationships differ across space, time, geography, and individuals?
		\end{itemize}
	\item Not going to talk about qualitative or case comparisons
	\end{itemize}
}


\subsection{Repeated Cross-Sections}
\frame{\tableofcontents[currentsection]}

\frame{
\frametitle{Repeated Cross-Section}
\begin{itemize}\itemsep1em
\item Simplest elaboration of a single survey
\item Draw multiple, independent samples at different points in time
	\begin{itemize}
	\item Note: in small populations, this can be complicated
	\end{itemize}
\item Useful for:
	\begin{itemize}
	\item Establishing trends/changes
	\item Comparing multivariate relationships over time
	\end{itemize}
\end{itemize}
}

\frame{
	\frametitle{Example: Election Polling}
	\begin{itemize}\itemsep1em
	\item Commonly used to study elections
	\item Highly inconsistent procedures
		\begin{itemize}
		\item Varying sample sizes
		\item Varying sampling procedures
		\item Varying question wordings
		\item Irregular field periods
		\end{itemize}
	\item Some examples:
		\begin{itemize}
		\item \href{http://elections.huffingtonpost.com/pollster}{Pollster}
		\item \href{http://lordashcroftpolls.com/}{Lord Ashcroft Polls}
		\end{itemize}
	\end{itemize}
}

\frame{
	\frametitle{Example: BES CMS}
	\begin{itemize}\itemsep1em
	\item Continuous Monitoring Study
	\item Procedures
		\begin{itemize}
		\item Monthly interviewing 2004--Present
		\item Modest sample sizes
		\item Very short questionnaire
		\end{itemize}
	\item Inspired ``rolling thunder'' study of 2015 campaign
	\item Data: \url{http://www.bes2009-10.org/cms-data.php}
	\end{itemize}
}



\frame{

\frametitle{Practical Considerations}

\begin{itemize}\itemsep1em
\item<2-> Are patterns of nonresponse consistent across periods?
\item<3-> What happens if we want/need to change questions?
\item<4-> What happens if we want/need to change sampling methods?
\item<5-> What if we want to draw individual-level comparisons?
\end{itemize}

}



\subsection{Survey Panels}
\frame{\tableofcontents[currentsection]}

\frame{
\frametitle{Panel Studies}

\begin{itemize}\itemsep1em
\item A panel is a single cohort, observed at multiple points in time
\item Synonyms: Within-subjects studies, longitudinal studies, cohort studies
\item First use: Lazarsfeld and Fiske (1938)
\end{itemize}

}

\frame{
	\frametitle{{\normalsize Ex: ``Classic'' Election Studies}}
	\begin{itemize}\itemsep1em
	\item ``Columbia School'': Lazarsfeld et al. (1948) and Berelson et al. (1954)%\footnote{Lazarsfeld, P. F.; Berelson, B. R. \& Gaudet, H. (1948), \textit{The People's Choice: How the Voter Makes up His Mind in a Presidential Campaign}, Columbia University Press. Berelson, B. R.; Lazarsfeld, P. F. \& McPhee, W. N. (1954), \textit{Voting: A Study of Opinion Formation in a Presidential Campaign}, The University of Chicago Press.}
	\item Empaneled small samples (less than n=1000)
	\item Measured vote intentions and other factors multiple times
	\item Largely found that campaigns didn't matter
	\end{itemize}
}

\frame{
	\frametitle{{\normalsize Ex: Modern Election Studies}}
	\begin{itemize}\itemsep1em
	\item American National Election Studies 
		\begin{itemize}
		\item Collected biennially (or more) since 1948
		\item Questionnaire is basically unchanged
		\item Most years have pre/post interviews
		\item Profound source of data, survey innovations, \& experiments
		\item \url{http://electionstudies.org/}
		\end{itemize}
	\item National Annenberg Election Survey
		\begin{itemize}
		\item Five-wave panels during 2004 and 2008 U.S. Presidential elections
		\item Online and telephone interviewing
		\item Intended to mimic ``classic'' studies
		\item \href{http://www.annenbergpublicpolicycenter.org/political-communication/naes/}{Click for more info}
		\end{itemize}
	\end{itemize}
}


\frame{
\frametitle{Trade-offs}
\begin{itemize}\itemsep1em
\item<2-> Advantages
	\begin{itemize}
	\item Within-subjects comparisons
	\item Statistical efficiency
	\item Cost?
	\end{itemize}
\item<3-> Disadvantages
	\begin{itemize}
	\item Initial recruitment
	\item Attrition (and incentivization)
	\item Panel conditioning
	\item Cost?
	\end{itemize}
\end{itemize}

}

\frame{

	\frametitle{Statistical Advantages I}
	
	\begin{itemize}
	\item Independent samples\footnote{Note: We \textit{do not} actually need the raw data to calculate this.}
	   \begin{align*}
	   \beta &= \dfrac{\sum_{i=1}^{n_1} x_{i1}}{n_1} - \dfrac{\sum_{i=1}^{n_0} x_{i0}}{n_0} \\
	   SE_{\beta} &= \sqrt{\dfrac{s_1^2}{n_1} + \dfrac{s_0^2}{n_0}}
	   \end{align*}
	\end{itemize}
}

\frame{

	\frametitle{Statistical Advantages II}
	
	\begin{itemize}
	\item Paired samples\footnote{Note: We \textit{do} need the raw data to calculate this.}
		 \begin{align*}
		 \beta &= \dfrac{\sum_{i=1}^{n} (x_{i1} - x_{i0})}{n} \\
		 SE_{\beta} &= \sqrt{\dfrac{{s_{Diff}}^2}{n}}
		 \end{align*}
	\item Standard error for paired samples decreases as correlation between $t_0$ and $t_1$ observations increases.
	\end{itemize}
}



\frame{

\frametitle{Panel Variants}

\begin{itemize}\itemsep1em
\item Rolling panel
\item Replenishment samples
\item Subpanels
\item ``Online panels''
\end{itemize}

}

\frame{}

\subsection{Multi-Level/Comparative Data}
\frame{\tableofcontents[currentsection]}

\frame{
\frametitle{Multi-Level Data}

\begin{itemize}\itemsep1em
\item Multi-level data collection involves sampling within different geographical units
\item Depending on research goal, this can be thought of as either stratified sampling or cluster sampling, or both
	\begin{itemize}
	\item Stratified sampling: Geography correlates with variables of interest and we desire large, representative samples within each geographical stratum
	\item Cluster sampling: Goal is to make claims about ``super population'' and units are necessary for interviewing procedures
	\end{itemize}
\item Often used in cross-national comparative analyses
\end{itemize}
}

\frame{

	\frametitle{{\normalsize Ex: European Social Survey}}
	
	\begin{itemize}\itemsep1em
	\item Biennial cross-sectional survey conducted since 2001
		\begin{itemize}
		\item Currently round 8 in field
		\end{itemize}
	\item Implemented in parallel across \href{http://www.europeansocialsurvey.org/about/participating_countries.html}{most European countries}
	\item Extremely high quality
		\begin{itemize}
		\item CAPI interviewing
		\item Probability based samples
		\item Precise translation
		\end{itemize}
	\item \url{http://www.europeansocialsurvey.org/}
	\end{itemize}
}


\frame{

	\frametitle{{\normalsize Ex: World Values Survey}}
	
	\begin{itemize}\itemsep1em
	\item Periodic survey since 1985
		\begin{itemize}
		\item Currently planning Wave 8 for 2016--2018
		\item Long field period (2+ years)
		\item Approximately 100 countries
		\end{itemize}
	\item Well-conducted
		\begin{itemize}
		\item CAPI or CATI interviewing
		\item Mostly probability samples
		\item More cross-national protocol variations than ESS
		\end{itemize}
	\item {\footnotesize \url{http://www.worldvaluessurvey.org/wvs.jsp}}
	\end{itemize}
}




\section{Regression Analysis}
\frame{\tableofcontents[currentsection]}


\frame{
	\frametitle{Uses of Regression}
	\begin{enumerate}\itemsep2em
	\item Description
	\item Prediction
	\item Causal Inference
	\end{enumerate}
}

\frame{
	\frametitle{Descriptive Inference}
	\begin{enumerate}\itemsep1em
	\item We want to understand a \textit{population} of cases
	\item We cannot observe them all, so:
		\begin{enumerate}
    		\item Draw a \textit{representative} sample
    		\item Perform mathematical procedures on sample data
    		\item Use assumptions to make inferences about population
    		\item Express uncertainty about those inferences based on assumptions
		\end{enumerate}
	\end{enumerate}
}

\frame{
	\frametitle{Parameter Estimation}
	\begin{itemize}\itemsep1em
    	\item We want to observe population \textit{parameter} $\theta$
    	\item If we obtain a representative sample of population units:\\
    		\begin{itemize}\itemsep1em
        		\item Our sample statistic $\hat{\theta}$ is an unbiased estimate of $\theta$
        		\item Our sampling procedure dictates how uncertain we are about the value of $\theta$
    		\end{itemize}
	\end{itemize}
}


\frame{
	\frametitle{Causal Inference}
	\begin{enumerate}\itemsep1em
	\item<2-> Everything that goes into descriptive inference
	\item<3-> Plus, philosophical assumptions
	\item<4-> Plus, randomization \textit{or} perfectly specified model
	\end{enumerate}
}



\subsection{OLS}
\frame{\tableofcontents[currentsection]}


\frame{
\frametitle{Relationship}

\begin{itemize}\itemsep1em
\item<1-> Covariance:\\
	$Cov(X,Y) = \sum_{i=1}^{n} \dfrac{(X_i - \bar{X})(Y_i - \bar{Y})}{n-1}$
\item<2-> Pearson's Correlation:\\
	{\small $Corr(X,Y) = r_{x,y} = \sum_{i=1}^{n} \dfrac{(X_i - \bar{X})(Y_i - \bar{Y})}{(n-1)s_x s_y}$}
\end{itemize}
}


\frame{

\frametitle{Correlation is linear!}

\includegraphics[width=\textwidth]{images/correlation}

\vspace{1em}
{\footnotesize Source: \href{https://commons.wikimedia.org/wiki/File:Correlation_examples2.svg}{Wikimedia}}
}


\frame{

	\frametitle{Analyzing Complex Surveys}
	
	\small
	
	\begin{itemize}\itemsep1em
	\item There's a saying: ``Every simple random survey is simple in the same way, but every complex survey is complex in its own way.''
	\item<2-> Statistics courses will almost always assume simple random sampling
	\item<3-> Any sample that is not self-weighting requires more complicated \textit{estimators} that account for varying weights
	\item<4-> Don't try to do this by hand
		\begin{itemize}
 		\item \href{http://www.stata.com/manuals13/svy.pdf}{Stata \texttt{svy} module}
 		\item \href{https://cran.r-project.org/web/packages/survey/index.html}{R \texttt{survey} package}
		\end{itemize}
	\end{itemize}

}


\frame<1-5,2>[label=ways]{
	\frametitle{Ways of Thinking About OLS}
	\begin{enumerate}\itemsep1em
    	\item<2-> Estimating Unit-level Causal Effect
    	\item<3-> Ratio of $Cov(X,Y)$ and $Var(X)$
    	\item<4-> Minimizing residual sum of squares (SSR)
    	\item<5-> Line (or surface) of best fit
	\end{enumerate}
}


\frame{
	\frametitle{Bivariate Regression I}
	\begin{itemize}\itemsep1em
	\item $Y$ is continuous
	\item $X$ is a randomized treatment indicator/dummy $(0,1)$
	\item How do we know if the treatment $X$ had an effect on $Y$?
	\item<2-> Look at mean-difference: $E[Y_i|X_i=1] - E[Y_i|X_i=0]$
	\end{itemize}
}


\frame{
	\frametitle{Three Equations}
	\begin{enumerate}\itemsep2em
	\item Population: $Y = \beta_0 + \beta_1 X \hspace{0.5em} (+ \epsilon)$
	\item Sample estimate: $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$
	\item Unit:
		\begin{align*}
		y_i & = \hat{\beta}_0 + \hat{\beta}_1 x_i + e_i\\
		    & = \bar{y}_{0i} + (y_{1i} - y_{0i}) x_i + (y_{0i} - \bar{y}_{0i})
		\end{align*}
	\end{enumerate}
}


\frame<1-2>[label=dummy]{
	\frametitle{Bivariate Regression I}
	\begin{itemize}\itemsep1em
    	\item Mean difference ($E[Y_i|X_i=1] - E[Y_i|X_i=0]$) is the regression line slope
    	\item Slope ($\beta$) defined as $\frac{\Delta Y}{\Delta X}$
    		\vspace{1em}
    		\begin{itemize}\itemsep1em
        		\item<2-> $\Delta Y = E[Y_i|X=1] - E[Y_i|X=0]$
        		\item<2-> $\Delta X = 1 - 0 = 1$
    		\end{itemize}
    	\item<3-> How do we know if this is a \textit{significant} difference?
    		\begin{itemize}
        		\item We'll come back to that
    		\end{itemize}
	\end{itemize}

}

% graphs of mean-difference
% http://thomasleeper.com/regcourse/Slides-2014/Session02_01.html#20


\frame[label=bivariate1]{
	\begin{center}
	\begin{tikzpicture}[>=latex', scale=0.8]
        \draw[->] (0,0) node[below] (origin) {}  -- (8,0) node[right] (xaxis) {x};
        \draw[->] (origin) -- (0,8) node[left] (yaxis) {y};
        % x ticks
        \draw (2,1pt) -- (2,-3pt) node[anchor=north] {0};
        \draw (6,1pt) -- (6,-3pt) node[anchor=north] {1};
        % y ticks
        \foreach \y in {1,...,7}
             \draw (1pt,\y) -- (-3pt,\y) node[anchor=east] {$\y$};
        
        % points
        \foreach \y in {0.5,1.0,...,3.5} {
        	\draw[gray,fill] (2,\y) circle [radius=1pt];
        	\draw[gray,fill] (6,3+\y) circle [radius=1pt];
        }
        % y_0-bar
        \draw<2-4>[dashed] (0,2) -- (8,2) node[right] {$\bar{y_0}$};
        % y_1-bar
        \draw<2-4>[dashed] (0,5) -- (8,5) node[right] {$\bar{y_1}$};
        % mean points
        \draw<2->[red,fill] (2,2) circle [radius=3pt];
        \draw<2->[red,fill] (6,5) circle [radius=3pt];
        
        % slope
        \draw<3-4>[solid, line width=2pt] (2,2) -- (6,2);
        \draw<3-5>[solid, line width=2pt] (6,2) -- (6,5);
        \node<3-4>(deltax) at (4,1) {$\Delta_x$};
        \node<3>[right](deltay) at (7,3.5) {$\Delta_y$};
        \node<4>[right](deltay) at (7,3.5) {$\Delta_y = \beta_1$};
        \node<5>[right](deltay) at (7,3.5) {$\hat{\beta}_1$};
        
        \draw<4->[blue, solid, line width=2pt] (2,2) -- (6,5);
        \node<4-5>[below left](b0) at (2,2) {$\hat{\beta}_0$};
        
        \node<5>[right](eq) at (0.5,7) {\small $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$};
        \node<6->[right](eq) at (0.5,7) {\small $\hat{y} = 2 + 3 x$};
        \node<7->[right](eq) at (0.5,6) {\small $y_i = 2 + 3 x_i + e_i$};
        \draw<7->[right,decorate,decoration={brace,mirror,amplitude=7.5pt}] (6,5)  -- (6,6.5)
        	node [right, pos=0.5, xshift=7] {\small $e_i$};
    \end{tikzpicture}
    \end{center}
}

% OLS only makes sense in a linear world

\frame{
	\frametitle{Systematic versus unsystematic component of the data}
	\begin{itemize}\itemsep1em
    	\item Systematic: Regression line (slope)
    		\begin{itemize}
    		\item Linear regression estimates the conditional means of the population data (i.e., $E[Y|X]$)
    		\end{itemize}
    	\item Unsystematic: Error term is the deviation of observations from the line
    		\begin{itemize}
        		\item The difference between each value $y_i$ and $\hat{y}_i$ is the \textit{residual}: $e_i$
        		\item OLS produces an estimate of the relationship between X and Y that minimizes the \textit{residual sum of squares}
    		\end{itemize}
	\end{itemize}
}

\frame{
	\frametitle{Why are there residuals?}
	\begin{itemize}\itemsep1em
		\item<2-> Omitted variables
		\item<2-> Measurement error
		\item<2-> Fundamental randomness
	\end{itemize}
}

\againframe<2-3>{dummy}

\againframe<2-3>{ways}


\frame{
	\frametitle{Bivariate Regression II}
	\begin{itemize}\itemsep1em
	\item $Y$ is continuous
	\item $X$ is continuous (and randomized)
	\item How do we know if the treatment $X$ had an effect on $Y$?\\
		\begin{itemize}
		\item Correlation coefficient ($\rho$)
		\item Regression coefficient (slope; $\beta_1$)
		\end{itemize}
	\end{itemize}
}

\frame<1>[label=correlation]{
	\frametitle{Correlation Coefficient ($\rho$)}
	\begin{itemize}\itemsep1em
	\item Measures how well a scatterplot is represented by a straight (non-horizontal) line
	\item<2-> Formal definition: 
		$\frac{Cov(X,Y)}{\sigma_X \sigma_y}$
	\item<2-> As a reminder:\\
		\begin{itemize}\itemsep1em
		\item $Cov(x,y) = \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})$
		\item $s_x = \sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}$
		\end{itemize}
	\end{itemize}
}

\frame{
	\includegraphics[width=\textwidth]{images/correlation}
}

\againframe<1->{correlation}

\frame{
	\frametitle{OLS Coefficient ($\beta_1$)\footnote{Multivariate formula involves matrices; Week 20}}
	\begin{itemize}\itemsep1em
	\item Measures $\Delta Y$ given $\Delta X$
	\item<2-> Formal definition: 
		$\frac{Cov(X,Y)}{Var(X)}$
	\item<2-> As a reminder:\\
		\begin{itemize}\itemsep1em
		\item $Cov(x,y) = \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})$
		\item $Var(x) = \sum_{i=1}^{n}(x_i - \bar{x})^2$
		\end{itemize}
	\item<3-> $\hat{\rho}$ and $\hat{\beta_1}$ are just scaled versions of $\widehat{Cov}(x,y)$
	\end{itemize}
}

\frame<1-6>[label=math]{
	\frametitle{Minimum Mathematical Requirements}
	\begin{enumerate}\itemsep1em
    	\item<1-> Do we need variation in $X$?
    		\begin{itemize}
        		\item<2-> Yes, otherwise dividing by zero
    		\end{itemize}
    	\item<3-> Do we need variation in $Y$?
    		\begin{itemize}
        		\item No, $\hat{\beta}_1$ can equal zero
    		\end{itemize}
    	\item<5-> How many observations do we need?
    		\begin{itemize}
    		\item<6-> $n \ge k$, where $k$ is number of parameters to be estimated
    		\end{itemize}
    	\item<7-> Can we have highly correlated regressors?
    		\begin{itemize}
    		\item<8-> Generally no (due to multicollinearity)
    		\end{itemize}
	\end{enumerate}
}

\frame<1-5>[label=scatter]{
	\begin{center}
	\begin{tikzpicture}[>=latex', scale=0.8]
        \draw[->] (0,0) node[below] (origin) {0}  -- (8,0) node[right] (xaxis) {x};
        \draw[->] (origin) -- (0,8) node[left] (yaxis) {y};
        % x ticks
        \foreach \x in {1,...,7}
        	\draw (\x,1pt) -- (\x,-3pt) node[anchor=north] {$\x$};
        % y ticks
        \foreach \y in {1,...,7}
             \draw (1pt,\y) -- (-3pt,\y) node[anchor=east] {$\y$};
        % points
        \draw[fill] (1,1) circle [radius=1.5pt];
        \draw[fill] (2,5) circle [radius=1.5pt];
        \draw[fill] (3,3) circle [radius=1.5pt];
        \draw[fill] (4,6) circle [radius=1.5pt];
        \draw[fill] (5,2) circle [radius=1.5pt];
        \draw[fill] (6,7) circle [radius=1.5pt];
        % x-bar
        \draw<2->[dashed] (3.5, 0) -- (3.5,8) node[above] {$\bar{x}$};
        % y-bar
        \draw<2->[dashed] (0,4) -- (8,4) node[right] {$\bar{y}$};
        
        % x-deviations
        \draw<3>[red, line width=2pt] (1,1) -- (3.5,1);
        \draw<3>[red, line width=2pt] (2,5) -- (3.5,5);
        \draw<3>[red, line width=2pt] (3,3) -- (3.5,3);
        \draw<3>[red, line width=2pt] (4,6) -- (3.5,6);
        \draw<3>[red, line width=2pt] (5,2) -- (3.5,2);
        \draw<3>[red, line width=2pt] (6,7) -- (3.5,7);
        % y-deviations
        \draw<4,6-7,10-11>[red, line width=2pt] (1,1) -- (1,4);
        \draw<4,6-7,10-11>[red, line width=2pt] (2,5) -- (2,4);
        \draw<4,6-7,10-11>[red, line width=2pt] (3,3) -- (3,4);
        \draw<4,6-7,10-11>[red, line width=2pt] (4,6) -- (4,4);
        \draw<4,6-7,10-11>[red, line width=2pt] (5,2) -- (5,4);
        \draw<4,6-7,10-11>[red, line width=2pt] (6,7) -- (6,4);
        
		\fill<7,11>[red, opacity=0.5] (1,1) rectangle (4,4);
		\fill<7,11>[red, opacity=0.5] (2,5) rectangle (3,4);
		\fill<7,11>[red, opacity=0.5] (3,3) rectangle (4,4);
		\fill<7,11>[red, opacity=0.5] (4,6) rectangle (6,4);
		\fill<7,11>[red, opacity=0.5] (5,2) rectangle (7,4);
		\fill<7,11>[red, opacity=0.5] (6,7) rectangle (9,4);

        % line
        \draw<5->[blue, line width=2pt] (0,1.6) -- (8,7.0856);
        
        % residuals
        \draw<8->[blue, line width=1pt] (1,1) -- (1,2.29);
        \draw<8->[blue, line width=1pt] (2,5) -- (2,2.97);
        \draw<8->[blue, line width=1pt] (3,3) -- (3,3.66);
        \draw<8->[blue, line width=1pt] (4,6) -- (4,4.34);
        \draw<8->[blue, line width=1pt] (5,2) -- (5,5.03);
        \draw<8->[blue, line width=1pt] (6,7) -- (6,5.71);
		
		\fill<9-11>[blue, opacity=0.5] (1,1) rectangle (2.29,2.29);
		\fill<9-11>[blue, opacity=0.5] (2,5) rectangle (4.03,2.97);
		\fill<9-11>[blue, opacity=0.5] (3,3) rectangle (3.66,3.66);
		\fill<9-11>[blue, opacity=0.5] (4,6) rectangle (5.66,4.34);
		\fill<9-11>[blue, opacity=0.5] (5,2) rectangle (8.03,5.03);
		\fill<9-11>[blue, opacity=0.5] (6,7) rectangle (7.29,5.71);

		
    \end{tikzpicture}
    \end{center}
}

\frame{
	\frametitle{Calculations}
	\begin{tabular}{rrrrrr} \hline
	$x_i$ & $y_i$ & $x_i - \bar{x}$ & $y_i - \bar{y}$ & $(x_i - \bar{x})(y_i - \bar{y})$ & $(x_i - \bar{x})^2$\\ \hline
	1 & 1 & ? & ? & ? & ? \\
	2 & 5 & ? & ? & ? & ? \\
	3 & 3 & ? & ? & ? & ? \\
	4 & 6 & ? & ? & ? & ? \\
	5 & 2 & ? & ? & ? & ? \\
	6 & 7 & ? & ? & ? & ? \\ \hline
	\end{tabular}
}
% mean(x) = 3.5
% mean(y) = 4
% cov(x,y) = 12
% var(x) = 17.5
% slope = 0.6857143

\frame{
	\frametitle{Intercept $\hat{\beta}_0$}
	\begin{itemize}\itemsep1em
	\item Simple formula: $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$
	\item<2-> Intuition: OLS fit always runs through point $(\bar{x}, \bar{y})$
	\item<3-> Ex.: $\hat{\beta}_0 = 4 - 0.6857 * 3.5 = 1.6$
	\item<4-> $\hat{y} = 1.6 + 0.6857 \hat{x}$
	\end{itemize}
}

\againframe<5>{scatter}

% CEF: compare $\bar{Y}$ and $E[Y|X=x]$

\againframe<3-4>{ways}


\frame{
	\frametitle{OLS Minimizes SSR}
	\begin{itemize}\itemsep1em
    	\item Total Sum of Squares (SST): $\sum_{i=1}^{n}(y_i - \bar{y})^2$
    	\item We can partition SST into two parts (ANOVA):
    		\begin{itemize}
    		\item Explained Sum of Squares (SSE)
    		\item Residual Sum of Squares (SSR)
    		\end{itemize}
    	\item $SST = SSE + SSR$
    	\item OLS is the line with the lowest SSR
	\end{itemize}
}

\againframe<5-11>{scatter}


\frame[label=anygood]{
	\frametitle{Are Our Estimates Any Good?}
	Yes, if:\\
	\begin{enumerate}\itemsep1em
   		\item Works mathematically
   		\item Causally valid theory
   		\item Linear relationship between $X$ and $Y$
   		\item $X$ is measured without error
   		\item No missing data (or MCAR)
   		\item No confounding
	\end{enumerate}
}


\frame{
	\frametitle{Linear Relationship}
	\begin{itemize}\itemsep1em
    	\item If linear, no problems
    	\item If non-linear, we need to transform
    		\begin{itemize}
        		\item Power terms (e.g., $x^2$, $x^3$)
        		\item log (e.g., $log(x)$)
        		\item Other transformations
        		\item If categorical: convert to set of indicators
        		\item Multivariate interactions (next week)
    		\end{itemize}
	\end{itemize}
}

\frame{
	\frametitle{Coefficient Interpretation}
	\begin{itemize}\itemsep1em
	\item Four types of variables:
		\begin{enumerate}
		\item Indicator (0,1)
		\item Categorical
		\item Ordinal
		\item Interval
		\end{enumerate}
	\item How do we interpret a coefficient on each of these types of variables?
	\end{itemize}
}

\frame{
	\frametitle{Notes on Interpretation}
	\begin{itemize}\itemsep1em
	\item<1-> Effect $\beta_1$ is constant across values of $x$
	\item<2-> That is not true when there are:
		\begin{itemize}
		\item Interaction terms (next week)
		\item Nonlinear transformations (e.g., $x^2$)
		\item Nonlinear regression models (e.g., logit/probit)
		\end{itemize}
	\item<3-> Interpretations are sample-level
		\begin{itemize}
		\item Sample representativeness determines generalizability
		\end{itemize}
	\item<4-> Remember uncertainty
		\begin{itemize}
		\item These are \textit{estimates}, not population parameters
		\end{itemize}
	\end{itemize}
}



\frame{
	\frametitle{Measurement Error in Regressor(s)}
	\begin{itemize}\itemsep1em
    	\item We want effect of $x$, but we observe $x^{*}$, where $x = x^{*} + w$:
    	\begin{align*}
    	y & = \beta_0 + \beta_1 x^{*} + \epsilon\\
    	 & = \beta_0 + \beta_1 (x - w) + \epsilon\\
    	 & = \beta_0 + \beta_1 x + (\epsilon - \beta_1 w)\\
    	 & = \beta_0 + \beta_1 x + v
    	\end{align*}
	\end{itemize}
}

\frame{
	\frametitle{Measurement Error in Regressor(s)}
	\begin{itemize}\itemsep1em
    	\item Produces \textit{attenuation}: as measurement error increases, $\beta_1 \rightarrow 0$
    	\item Our coefficients fit the observed data
    	\item But they are \textit{biased} estimates of our population equation
    		\begin{itemize}
    		\item This applies to all $\hat{\beta}$ in a multivariate regression
    		\item Direction of bias is unknown
    		\end{itemize}
	\end{itemize}
}


\frame{
	\frametitle{Measurement Error in $Y$}
	\begin{itemize}\itemsep1em
    	\item Not necessarily a problem
    	\item If \textit{random} (i.e., uncorrelated with $x$), it costs us precision
    	\item If \textit{systematic}, who knows?!
	\end{itemize}
}


\frame{
	\frametitle{Confounding (Selection Bias)}
	\begin{itemize}\itemsep1em
	\item If $x$ is not randomly assigned, potential outcomes are not independent of $x$
	\item Other factors explain why a unit $i$ received their particular value $x_i$
	\vspace{1em}
	\item In matching, we obtain this \textit{conditional independence} by comparing units that are identical on all confounding variables
	\end{itemize}
}

\frame{
	\frametitle{Omitted Variables}
	{\small
	$\underbrace{E[Y_i| X_i = 1] - E[Y_i | X_i = 0] =}_{\text{Naive Effect}}$\\
	\vspace{1em}
	$\underbrace{E[Y_{1i}|X_i =1] - E[Y_{0i} | X_i = 1]}_{\text{Treatment Effect on Treated (ATT)}} + \underbrace{E[Y_{0i}|X_i = 1] - E[Y_{0i}|X_i=0]}_{\text{Selection Bias}}$
	}
	\vspace{1em}
}

\frame[label=causalgraph]{
	\begin{center}
	\begin{tikzpicture}[>=latex',circ/.style={draw, shape=circle, node distance=5cm, line width=1.5pt}]
        \draw[->] (0,0) node[left] (X) {X} -- (2.5,0) node[right] (D) {D};
        \draw[->] (3.1,0) -- (5,0) node[right] (Y) {Y};
        \draw[->] (-3,4) node[above] (Z) {Z} -- (X);
        \draw[->] (Z) -- (Y);
        \draw[->] (5,2) node[above] (A) {A} -- (Y);
        \draw[->] (-2,0) node[left] (B) {B} -- (X);
        \draw[->] (X) -- (2,-2) node[right] (C) {C};
    \end{tikzpicture}
    \end{center}
}

\frame{
	\frametitle{Omitted Variable Bias}
	\begin{itemize}
	\item We want to estimate:
	\begin{align*}
	Y = \beta_0 + \beta_1 X + \beta_2 Z + \epsilon
	\end{align*}
	\item We actually estimate:
	\begin{align*}
	\tilde{y} & = \tilde{\beta_0} + \tilde{\beta_1} x + \epsilon\\
	& = \tilde{\beta_0} + \tilde{\beta_1} x + (0 * z) + \epsilon\\
	& = \tilde{\beta_0} + \tilde{\beta_1} x + \nu
	\end{align*}
	\item Bias: $\tilde{\beta}_1 = \hat{\beta}_1 + \hat{\beta}_2 \tilde{\delta}_1$, where $\tilde{z} = \tilde{\delta}_0 + \tilde{\delta}_1 x$
	\end{itemize}
}

\frame{
	\frametitle{Size and Direction of Bias}
	\begin{itemize}
	\item Bias: $\tilde{\beta}_1 = \hat{\beta}_1 + \hat{\beta}_2 \tilde{\delta}_1$, where $\tilde{z} = \tilde{\delta}_0 + \tilde{\delta}_1 x$
	\end{itemize}
	\vspace{1em}
	\begin{center}
	\begin{tabular}{lll}
		              & $Corr(x,z)<0$ & $Corr(x,z)>0$ \\ \hline
		$\beta_2 < 0$ & Positive      & Negative      \\
		$\beta_2 > 0$ & Negative      & Positive      \\ \hline
	\end{tabular}
	\end{center}
}


\frame{
	\frametitle{Aside: Three Meanings of ``Endogeneity''}
	Formally endogeneity is when $Cov(X,\epsilon) \neq 0$
	\vspace{1em}
	\begin{enumerate}\itemsep1em
	\item Measurement error in regressors
	\item Omitted variables associated with included regressors 
		\begin{itemize}
		\item ``Specification error''
		\item Confounding
		\end{itemize}
	\item Lack of temporal precedence
	\end{enumerate}
}




\frame{
	\frametitle{Common Conditioning Strategies}
	\begin{enumerate}\itemsep1em
    	\item<2-> Condition on nothing (``naive effect'')
    	\item<3-> Condition on some variables
    	\item<4-> Condition on all observables
	\end{enumerate}
	\vspace{1em}
	\onslide<5->{Which of these are good strategies?}
}


\frame<1-2>[label=what]{
	\frametitle{What goes in our regression?}
	\begin{itemize}\itemsep1em
    	\item Use theory to build causal models
    		\begin{itemize}
    		\item Often, a causal graph helps
    		\end{itemize}
    	\item Some guidance:
    		\begin{itemize}
    		\item<2-> Include confounding variables
	    	\item<3-> Do not include post-treatment variables
	    	\item<4-> Do not include \textit{colinear} variables
    		\item<5-> Including irrelevant variables costs certainty
    		\item<6-> Including variables that affect $Y$ alone increases certainty
    		\end{itemize}
	\end{itemize}
}

\againframe{causalgraph}

\againframe<2-3>{what}

\againframe{causalgraph}

\frame{
	\frametitle{Post-treatment Bias}
	\begin{itemize}\itemsep1em
	\item We usually want to know the \textbf{total effect} of a cause
	\item If we include a mediator, $D$, of the $X \rightarrow Y$ relationship, the coefficient on $X$:
		\begin{itemize}
    		\item Only reflects the \textbf{direct} effect
    		\item Excludes the \textbf{indirect} effect of $X$ through $M$
		\end{itemize}
	\item So don't control for mediators!
	\end{itemize}
}

\againframe<3-4>{what}

\againframe<6-8>{math}

\againframe<4-6>{what}

\againframe{causalgraph}

\frame{
	\frametitle{Multivariate Regression Interpretation}
	\begin{itemize}\itemsep1em
	\item All our interpretation rules from earlier still apply in a multivariate regression
	\item Now we interpret a coefficient as an effect ``all else constant''
	\item Generally, not good to give all coefficients a causal interpretation
		\begin{itemize}
		\item Think ``forward causal inference''
		\item We're interested in the $X \rightarrow Y$ effect
		\item All other coefficients are there as ``controls''
		\end{itemize}
	\end{itemize}
}

\frame{
	\frametitle{From Line to Surface I}
	\begin{itemize}\itemsep1em
	\item In simple regression, we estimate a \textbf{line}
	\item In multiple regression, we estimate a \textbf{surface}
	\item Each coefficient is the \textit{marginal effect}, all else constant (at mean)
	\item This can be hard to picture in your mind
	\end{itemize}
}

\frame{
	\frametitle{From Line to Surface II}
	\begin{center}
	\begin{tikzpicture}[scale=0.7]
	    \draw[thick,->] (0,0,0) -- (6,0,0) node[anchor=north east]{$x$};
	    \draw[thick,->] (0,0,0) -- (0,4,0) node[anchor=north west]{$y$};
	    \draw<1> (3,3,0) node {$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X$};
        \draw<2> (3,3,0) node {$\hat{y} = \hat{\beta}_0 + \hat{\beta}_2 Z$};
	    \draw<3> (3,3,0) node {$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X + \hat{\beta}_2 Z$};
	    \draw<1,3>[thick, blue] (0,2,0) -- (5,0,0);
	    \draw<2->[thick,->] (0,0,0) -- (0,0,5) node[anchor=south]{$z$};
	    \draw<2->[thick, blue] (0,2,0) -- (0,0,4);
	    \filldraw<3->[draw=red,fill=red!20,opacity=0.5]
	        (0,2,0) -- (0,0,4) -- (5,-2,4) -- (5,0,0) -- cycle;
	\end{tikzpicture}
	\end{center}
}


\againframe{anygood}

\frame{
	\frametitle{OLS is BLUE}
	\begin{itemize}\itemsep1em
	\item BLUE: Best Linear Unbiased Estimator
	\item Gauss Markov Assumptions:
		\begin{enumerate}
		\item Linearity in parameters
		\item Random sampling
		\item No multicollinearity
		\item Exogeneity ($E[\epsilon|\mathbf{X}] = 0$)
		\item Homoskedasticity ($Var(\epsilon|\mathbf{X}) = \sigma^2$)
		\end{enumerate}
	\item Assumptions 1--4 prove OLS is unbiased
	\item Assumption 5 proves OLS is the \textit{best} estimator
	\end{itemize}
}



\frame{
	\frametitle{Squared vs. Absolute Errors}
	\begin{itemize}\itemsep1em
	\item Conventionally use Sum of Squared Errors
	\item Using absolute errors is also unbiased
	\item Sum of Squared Errors:
		\begin{itemize}
		\item more heavily weights outliers
		\item has a smaller variance
		\end{itemize} 
	\item Thus OLS is \textbf{B}est\textbf{LUE}
	\end{itemize}
}







\subsection{Goodness-of-Fit}
\frame{\tableofcontents[currentsection]}

\frame{
	\frametitle{Goodness-of-Fit}
	\begin{itemize}\itemsep1em
	\item We want to know: ``How good is our model?''
	\item<2-> We can answer:\\
		``How well does our model fit the observed data?''
	\item<3-> Is this what we want to know?
	\end{itemize}
}

\frame{
	\frametitle{Correlation}
	\begin{itemize}\itemsep1em
	\item Definition: $Corr(x,y) = \hat{r}_{x,y} = \frac{Cov(x,y)}{(n-1) s_x s_y}$
	\item Slope $\hat{\beta}_1$ and correlation $\hat{r}_{x,y}$ are simply different scalings of $Cov(x,y)$
	\item Interpretation: How well the bivariate relationship is summarized by a cloud of points?
	\item Units: none (range -1 to 1)
	\end{itemize}
}

\frame{
	\frametitle{Coefficient of Determination ($R^2$)}
	
	\small
	
	\begin{itemize}\itemsep0.5em
	\item Definition: $R^2 = \hat{r}_{x,y}^2 = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}$
	\item Interpretation: How much of the total variation in $y$ is explained by the model?
	\item But, $R^2$ increases simply by adding more variables
	\item So, Adjusted-$R^2 = R^2 - (1 - R^2)\frac{k}{n-k-1}$, where $k$ is number of regressors
	\item Units: none (range 0 to 1)
	\end{itemize}
}

\frame{
	\frametitle{Standard Error of the Regression (SER)}
	\begin{itemize}\itemsep0.5em
	\item ``Root mean squared error'' or just $\sigma$
	\item Definition: $\hat{\sigma} = \sqrt{\frac{SSR}{n-p}}$, where $p$ is number of parameters estimated
	\item Interpretation: How far, on average, are the observed $y$ values from their corresponding fitted values $\hat{y}$
		\begin{itemize}
		\item $sd(y)$ is how far, on average, a given $y_i$ is from $\bar{y}$
		\item $\sigma$ is how far, on average, a given $y_i$ is from $\hat{y}_i$
		\end{itemize}
	\item Units: same as $y$ (range 0 to $sd(y)$)
	\end{itemize}
}

\frame{
	\frametitle{The F-test}
	\begin{itemize}\itemsep1em
	\item Definition: Test of whether any of our coefficients differ from zero
		\begin{itemize}
		\item In a bivariate regression, $F=t^2$
		\end{itemize}
	\item Interpretation: Do any of the coefficients differ from zero?
		\begin{itemize}
		\item Not a very interesting measure
		\end{itemize}
	\item Units: none (range 0 to $\infty$)
	\end{itemize}
}



\subsection{Generalized Linear Models}
\frame{\tableofcontents[currentsection]}



\frame{
    \frametitle{Non-continuous Outcomes}
    \begin{enumerate}\itemsep1em
    \item Why shouldn't we use OLS for a non-continuous outcome variable?
    \item<2-> What do we do instead?
        \begin{itemize}
        \item<3-> Use a generalized linear model (GLM)
        \end{itemize}
    \end{enumerate}
}


\frame{
    \frametitle{Regression on a Latent Variable}
    \begin{itemize}\itemsep1em
    \item Consider a binary outcome $y$ (e.g., voting)
    \item OLS provides a nonsensical fit to the outcome
    \item Think about the problem as a ``latent'' outcome ($y\ast$) that manifests in two observed categories
        \begin{itemize}
        \item As $y\ast$ increases, $Pr(Y=1) \rightarrow 1$
        \item As $y\ast$ decreases, $Pr(Y=1) \rightarrow 0$
        \end{itemize}
    \item We do not observe $y\ast$, only $y$
    \end{itemize}
}

\frame{
    \frametitle{Estimation in GLM}
    \begin{itemize}\itemsep1em
    \item In OLS, we estimate: $\hat{y} = \beta_0 + \beta_1 x + e$
    \item This represents the conditional mean of $y$
    \item In a GLM, we estimate: $\hat{y}\ast = \beta_0 + \beta_1 x + e$\\
    where $y\ast$ is a transformation of $y$
    \item This is also a prediction of the conditional mean of $y$
    \item<2-> How do we transform $y$ to $y\ast$?
    \end{itemize}
}

\frame{
    \frametitle{Model Specification}
    \begin{enumerate}\itemsep1em
    \item<1-> Complete set of conditioning variables
    \item<2-> Correctly specified model
    \item<3-> Choice of error distribution
    \item<4-> Link function
    \end{enumerate}
}

\frame{
	\frametitle{Error Distribution}
	
	\small
	
	\begin{itemize}\itemsep1em
	\item To estimate a linear model using OLS, no distributional assumption is needed
	\item We can use Maximum Likelihood Estimation to obtain identical coefficient estimates as OLS by assuming errors are Normally distributed
    	\begin{itemize}
    	\item $\dfrac{1}{\sigma\sqrt{2\pi}} e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}$
    	\end{itemize}
	\item For any GLM, we must assume the population distribution of the errors
    	\begin{itemize}
    	\item In almost all cases, an \textit{exponential family} distribution
    	\item i.e., a bell-shaped distribution that is not Normal
    	\end{itemize}
	\end{itemize}
}


\frame{
    \frametitle{Link Function}
    \begin{itemize}\itemsep2em
    \item $y\ast = X\beta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots$
    \item \textbf{Link function}: $g(\mu) = X\beta$
        \begin{itemize}
        \item Transforms $y$ to $y\ast$
        \end{itemize}
    \item \textbf{Inverse link function}: $\mu = g^{-1}(X\beta)$
        \begin{itemize}
        \item Transforms $y\ast$ back to $y$
        \end{itemize}
    \end{itemize}
}


\begin{frame}[fragile]
    \frametitle{Inverse Link Function}
    
    \begin{itemize}
    \item This plot displays $g^{-1}(0.75x)$, where $g^{-1}$ is the inverse logit link.
    \end{itemize}
    
    \begin{center}
    \begin{tikzpicture}[yscale=3.5, xscale=0.9]
    \draw [thin, step=0.1, lightgray] (-5,0) grid (5,1);
    \draw [thick, gray] (-5,0) grid (5,1);
    \node[right] () at (5,-5) {x};
    \node[right] () at (5,0) {$x$};
    \node[above] () at (-5,1) {$\Pr(y=1|x)$};
    \foreach \x in {-5,...,5} {
        \node[below] () at (\x,0) {\small \x};
    }
    \foreach \y in {0.0,0.2,0.4,0.6,0.8,1.0} {
    \node[left] () at (-5,\y) {\small \y};
    }
    % paste0("(", -5:5, ",", round(plogis(0.75*(-5:5)),4), ")", collapse = " -- ")
    \draw [very thick, blue] (-5,0.023) -- (-4,0.0474) -- (-3,0.0953) -- (-2,0.1824) -- (-1,0.3208) -- (0,0.5) -- (1,0.6792) -- (2,0.8176) -- (3,0.9047) -- (4,0.9526) -- (5,0.977);
    \end{tikzpicture}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{{\normalsize From $x$ to Linear Prediction ($y\ast$)}}
    \begin{center}
    \begin{tikzpicture}[scale=0.5]
    \draw [thin, step=0.1, lightgray] (-5,-5) grid (5,5);
    \draw [thick, gray] (-5,-5) grid (5,5);
    \node[right] () at (5,-5) {$x$};
    \node[above] () at (-5,5) {$y\ast$};
    \foreach \x in {-5,...,5} {
        \node[below] () at (\x,-5) {\footnotesize \x};
    }
    \foreach \y in {-5,...,5} {
    \node[left] () at (-5,\y) {\footnotesize \y};
    }
    % paste0("(", -5:5, ",", round(0.75*(-5:5),4), ")", collapse = " -- ")
    \draw [very thick, blue] (-5,-3.75) -- (-4,-3) -- (-3,-2.25) -- (-2,-1.5) -- (-1,-0.75) -- (0,0) -- (1,0.75) -- (2,1.5) -- (3,2.25) -- (4,3) -- (5,3.75);
    \node[right,blue] () at (5,3.75) {$0 + 0.75x$};
    \end{tikzpicture}
    \end{center}
\end{frame}


\begin{frame}[fragile]
    \frametitle{From $y\ast$ to $Pr(y=1)$}
    \begin{center}
    \begin{tikzpicture}[scale=0.75]
    \draw [thin, step=0.1, lightgray] (-5,-1) grid (5,2);
    \draw [thick, gray] (-5,-1) grid (5,2);
    \node[right] () at (5,-1) {$y\ast$};
    \node[above] () at (-5,2) {\small $\Pr(y=1|x)$};
    \foreach \x in {-5,...,5} {
        \node[below] () at (\x,-1) {\footnotesize \x};
    }
    \foreach \y in {-1,...,2} {
    \node[left] () at (-5,\y) {\footnotesize \y};
    }
    % paste0("(", -5:5, ",", round(plogis(0.75*(-5:5)),4), ")", collapse = " -- ")
    \draw [very thick, blue] (-5,0.023) -- (-4,0.0474) -- (-3,0.0953) -- (-2,0.1824) -- (-1,0.3208) -- (0,0.5) -- (1,0.6792) -- (2,0.8176) -- (3,0.9047) -- (4,0.9526) -- (5,0.977);
    \node[right,blue] () at (5,1) {\small $g^{-1}(0 + 0.75x)$};
    \end{tikzpicture}
    \end{center}
    
    \begin{itemize}
    \item Function is monotonic
    \item There is a \textit{cutpoint} in $y\ast$ where $\Pr(y) = 0.5$
    \item It is symmetric above and below the cutpoint
    \end{itemize}
\end{frame}


\begin{frame}[fragile]
    \frametitle{{\normalsize Putting it Together}}
    \vspace{-1.5em}
    \begin{center}
    \begin{tikzpicture}[scale=0.5]
    
    % Pr(y)
    \draw [thin, step=0.1, lightgray] (-12,-5) grid (-8,5);
    \draw [thick, gray] (-12,-5) grid (-8,5);
    \node[left,rotate=90] () at (-8,-5) {\footnotesize 0.00};
    \node[left,rotate=90] () at (-9,-5) {\footnotesize 0.25};
    \node[left,rotate=90] () at (-10,-5) {\footnotesize 0.50};
    \node[left,rotate=90] () at (-11,-5) {\footnotesize 0.75};
    \node[left,rotate=90] () at (-12,-5) {\footnotesize 1.00};
    \foreach \y in {-5,...,5} {
        \node[below,rotate=90] () at (-8,\y) {\footnotesize \y};
    }
    \node[right,rotate=90] () at (-8,5) {$y\ast$};
    \node[above right,rotate=90] () at (-12,-5) {\footnotesize $\Pr(y=1|x)$};
    % paste0("(", -(8 + 4*round(plogis(0.75*(-5:5)),4)), ",", -5:5, ")", collapse = " -- ")
    \draw [very thick, blue] (-8.092,-5) -- (-8.1896,-4) -- (-8.3812,-3) -- (-8.7296,-2) -- (-9.2832,-1) -- (-10,0) -- (-10.7168,1) -- (-11.2704,2) -- (-11.6188,3) -- (-11.8104,4) -- (-11.908,5);
        
    % y*
    \draw [thin, step=0.1, lightgray] (-5,-5) grid (5,5);
    \draw [thick, gray] (-5,-5) grid (5,5);
    \node[right] () at (5,-5) {$x$};
    \node[above] () at (-5,5) {$y\ast$};
    \foreach \x in {-5,...,5} {
        \node[below] () at (\x,-5) {\footnotesize \x};
    }
    \foreach \y in {-5,...,5} {
        \node[left] () at (-5,\y) {\footnotesize \y};
    }
    % paste0("(", -5:5, ",", round(0.75*(-5:5),4), ")", collapse = " -- ")
    \draw [very thick, blue] (-5,-3.75) -- (-4,-3) -- (-3,-2.25) -- (-2,-1.5) -- (-1,-0.75) -- (0,0) -- (1,0.75) -- (2,1.5) -- (3,2.25) -- (4,3) -- (5,3.75);
    %\node[right,blue] () at (5,3.75) {$0 + 0.75x$};
    \end{tikzpicture}
    \end{center}
\end{frame}


\frame{
	\frametitle{Choosing a Link Function}
	\begin{itemize}\itemsep1em
	\item Based on expected distribution of the error term of $y\ast$
	\item Choice heavily influenced by convention rather than empirics
	\item Choice of link adds \textit{model dependence}!
		\begin{itemize}
		\item Expected influence of $x$ on $y$ now depends on choice of link
		\item Different link functions can yield different substantive and statistical results
		\item Generally, results are similar
		\end{itemize}
	\end{itemize}
}


\frame{
    \frametitle{Common Link Functions}
    \begin{center}
    {\renewcommand{\arraystretch}{1.75}
    \begin{tabular}{p{1in} p{1in} p{1in}}
    \textit{Name} & \textit{Link} & \textit{Inverse} \\ \hline
    Identity & $\mu$ & $y\ast$ \\
    Logit & $\ln\dfrac{\mu}{1-\mu}$ & $\dfrac{1}{1+e^{-y\ast}}$ \\
    Probit & $\Phi^{-1}(\mu)$ & $\Phi(y\ast)$ \\
    \hline
    \end{tabular}
    }
    \end{center}
    
    \begin{itemize}
    \item These are common for categorical outcomes
    \item Other types of outcomes will use different link functions
    \end{itemize}
}


\frame{
    \frametitle{Beyond Binary Outcomes}
    \begin{itemize}\itemsep1em
    \item The generalized linear model works for all kinds of outcomes, not just continuous or binary
    \item Consider, for example, a multi-category, ordered outcome variable
    \item In an \textit{ordered logit} model, we imagine a latent variable $y\ast$ and multiple cutpoints between categories of $y$
    \end{itemize}
}


\frame{
    \frametitle{Maximum Likelihood Estimation}
    \begin{itemize}\itemsep1em
    \item The \textit{generalized linear model} is a way of describing complex regression models
    \item Unlike OLS, there is no closed-form mathematical solution to GLM
        \begin{itemize}
        \item Recall a linear model can be expressed as a GLM
        \end{itemize}
    \item GLMs involve the big additional assumption of a distribution for the error term
    \item Maximum likelihood estimation is a way of estimating the GLM that makes use of that error distribution
    \end{itemize}    
}

\frame{
    \frametitle{Maximum Likelihood Estimation}
    \begin{itemize}\itemsep0.5em
    \item Choose an error distribution (which is described by various parameters)
    \item Select parameters as starting values
    \item Give a \textit{probability} of seeing each observation in our sample data given that distribution
    \item Combine those probabilities (i.e., likelihoods)
        \begin{itemize}
        \item Multiply the likelihoods
        \item Add the log-likelihoods
        \end{itemize}
    \item Repeat and pick the best guess from all of those that we test
    \end{itemize}
}


% assumptions



\subsection{Interpreting GLMs}
\frame{\tableofcontents[currentsection]}

\frame{
    \frametitle{Coefficients}
    \begin{itemize}\itemsep1em
    \item Coefficients express effect of $x$ on $y\ast$
    \item In logistic regression, this is a statement about the odds-ratio:
        $\hat{\beta} = \dfrac{\frac{p_1}{1-p_1}}{\frac{p_0}{1-p_0}}$
    \item Coefficients are hard to interpret \textit{substantively}
    \item Statistical significance is similar to OLS
    \end{itemize}
}

\frame{
    \frametitle{Predicted Outcomes}
	\begin{itemize}\itemsep1em
	\item In OLS, fitted values from the estimated regression equation are values of $y$
	\item In GLMs, fitted values are expressed for $y\ast$
	\item To interpret logit or probit, we transform to predicted probabilities
	\end{itemize}
}

\frame{
    \frametitle{Predicted Outcomes}
	\begin{itemize}\itemsep1em
	\item Definition: According to our coefficient estimates, what is $\Pr(\hat{y} = 1|X)$?
	\item To calculate this, we:
    	\begin{enumerate}
    	\item Calculate a fitted value on the latent/linear scale
    	\item Plug that fitted value into the inverse link function
    	\end{enumerate}
    \item In Stata, use \texttt{margins} and \texttt{predict}
        \begin{itemize}
        \item Probabilities are the default
        \item Use the \texttt{, xb} option for linear predictions
        \end{itemize}
    \end{itemize}
}


\frame{
    \frametitle{Marginal Effects}
    \begin{itemize}\itemsep1em
    \item A marginal effect refers to one of two quantities:
        \begin{itemize}
        \item For continuous variables: the partial derivative of the regression equation with respect to a specific variable
        \item For categorical variables: the difference $\Pr(y=1|x=1) - \Pr(y=1|x=0)$
        \end{itemize}
    \item In an OLS (with no interactions or other complex terms), the marginal effect is the coefficient itself
    \item In GLMs, this is more complicated
    \end{itemize}
}


\frame{
    \frametitle{Review: Partial Derivatives}
    \begin{itemize}\itemsep1em
    \item The partial derivative is the \textit{instantaneous} slope (or \textit{tangent}) of a line
    \item With one $x$ variable, this is just the slope of the line
    \item With > 1 $x$ variable, this is the conditional slope of the regression surface
        \begin{itemize}
        \item We hold other variables at some value see a ``slice'' of the regression surface
        \item The marginal effect is the slope of the slice
        \end{itemize}
    \end{itemize}
}


\frame{
	\frametitle{Simple regression surface}
	\begin{center}
	\begin{tikzpicture}[scale=0.7]
	    \draw[thick,->] (0,0,0) -- (6,0,0) node[anchor=north east]{$x$};
	    \draw[thick,->] (0,0,0) -- (0,4,0) node[anchor=north west]{$y$};
	    \draw[thick,->] (0,0,0) -- (0,0,5) node[anchor=south]{$z$};
	    \filldraw[draw=red,fill=red!20,opacity=0.5]
	        (0,2,0) -- (0,0,4) -- (5,-2,4) -- (5,0,0) -- cycle;
	    \draw<1> (3,3,0) node {$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X + \hat{\beta}_2 Z$};
        \draw<2> (3,3,0) node {$\dfrac{\partial y}{\partial Z} = \hat{\beta}_2$};
	    \draw<2>[thick, red] (0,2,0) -- (0,0,4);
	    \draw<3> (3,3,0) node {$\dfrac{\partial y}{\partial X} = \hat{\beta}_1$};
	    \draw<3>[thick, blue] (0,2,0) -- (5,0,0);
	\end{tikzpicture}
	\end{center}
}



\frame{
    \frametitle{Discrete Changes}
    \begin{itemize}\itemsep1em
    \item Marginal effects are \textit{instantaneous changes}
    \item This makes sense for continuous variables
    \item For categorical (factor) variables, we often instead calculate a discrete change\\
        \begin{itemize}\itemsep1em
        \item $Pr(y=1|x=1) - Pr(y=1|x=0)$
        \item Marginal effect and discrete change are the same in OLS
        \end{itemize}
    \end{itemize}
}

\frame{
    \frametitle{Interaction Terms}
    \begin{itemize}\itemsep1em
    \item Due to the link function transformation, the marginal effect of $x$ depends on the value of $x$ and all other covariates
    \item This creates \textit{implicit} interactions
    \item We still have to include \textit{explicit} interaction terms to estimate heterogeneous effects (i.e., effect moderation)
    \end{itemize}
}


\frame{
	\frametitle{Logit vs. Probit}
	\begin{itemize}\itemsep1em
	\item<1-> Both constrain a continuous $y\ast$ to (0,1)
	\item<2-> Probabilities are symmetric
	\item<3-> Logit allows us to estimate odds-ratios
	\item<4-> Logit is maybe slightly more common in political science for what are probably just historical reasons
	\end{itemize}
}

\frame{
    \frametitle{Language of Interpretation}
    \begin{itemize}\itemsep0.5em
    \item How do we describe a marginal effect in OLS?
    \item<2-> In a binary outcome model, we use different language\\
    \item<3-> The substantive importance of an effect may depend on the level of $\Pr(y)$ at which it occurs\\
        \begin{itemize}\itemsep1em
        \item<4-> Small effect at $\Pr(y) = 0.01$ vs. $\Pr(y) = 0.48$
        \item<5-> Large positive effect when $\Pr(y)$ is always $> 0.6$
        \end{itemize}
    \item<6-> Substantive importance depends on variability and \textit{stickiness} of $x$
    \end{itemize}
}


\frame{
    \frametitle{Summarizing Marginal Effects}
    \begin{itemize}\itemsep1em
    \item We need to decide the values for all covariates that we will use in summarizing the marginal effect of our focal variable
    \item If our equation is: $y = g^{-1}(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots)$
    \item And we want to know the marginal effect of $x_1$, we need to hold $x_2$ at some specified value(s)
    \end{itemize}
}

\frame{
    \frametitle{3 Common Marginal Effect Summaries}
    \begin{enumerate}\itemsep2em
    \item Marginal Effects at the Mean (MEMs)
    \item Marginal Effects at Representative Values (MERs)
    \item Average Marginal Effects (AMEs)
    \end{enumerate}
}


\frame{
    \frametitle{MEM}
    \begin{itemize}\itemsep1em
    \item We are interested in the ME of $x_1$
    \item We hold all other covariates at their respective means
    \item For example, we interested in the ME of \textit{knowledge}, we hold \textit{education} at its mean
    \item<2-> Does this make sense for categorical values (e.g., gender)?
    \end{itemize}
}

\frame{
    \frametitle{MERs}
    \begin{itemize}\itemsep1em
    \item The means of the covariates may not be meaningful
    \item We hold those covariates at various interesting values
        \begin{itemize}
        \item ME of $x$ for a high-school educated female
        \item ME of $x$ for a university-educated male
        \item etc.
        \end{itemize}
    \item Helpful because we may also be interested in $\Pr(y=1)$ for these cases
    \end{itemize}
}

\frame{
    \frametitle{AMEs}
    \begin{itemize}\itemsep1em
    \item We may not only be interested in MEs at particular values
    \item We may want a summary measure of the effect of $x$ for our sample as a whole
    \item The \textit{average marginal effect} calculates the MER for every observation in our data, then averages those ME values
    \item This is Stata's default behavior when using:\\ \texttt{margins, dydx(*)}
    \end{itemize}
}


\frame{
    \frametitle{AMEs: An Example}
    \begin{itemize}
    \item Model effects of gender \& education on trust
    \item Calculate ME for each observation
    \item Average to obtain AME
    \end{itemize}
    
    \visible<2->{
    \begin{center}
    \begin{tabular}{lllrr}
    Obs. & Gender & Degree & $ME(Gender)$ & $ME(Degree)$ \\
    \hline 
    1 & 1 & 1 & 0.10 & 0.25 \\
    2 & 1 & 1 & 0.10 & 0.25 \\
    3 & 1 & 0 & 0.20 & 0.15 \\
    4 & 0 & 1 & 0.30 & -0.25 \\
    5 & 0 & 0 & 0.10 & -0.40 \\
    \hline 
    AME & -- & -- & \only<3->{0.16} & \only<4->{0.00} \\
    \hline 
    \end{tabular}
    \end{center}
    }
}

\frame{
    \frametitle{Statistical Uncertainty}
    \begin{itemize}\itemsep1em
    \item Always express statistical uncertainty for:
        \begin{itemize}
        \item Coefficients
        \item Predicted probabilities
        \item Marginal effects
        \end{itemize}
    \item Significance of coefficients and marginal effects may vary
    \item Marginal effects may only differ from 0 on a subset of the range of $x$
    \item Be cautious about extrapolation
    \end{itemize}
}


\frame{
    \frametitle{Aside: Discrete Effects}
    \begin{itemize}\itemsep1em
    \item Discrete effects can also be calculated for continuous variables
    \item Requires choosing a substantively meaningful change in $x$
    \item \textbf{Caution!} Not necessary equal:\\
        \begin{itemize}\itemsep0.5em 
        \item $\Pr(y = 1|x = 5) - \Pr(y = 1|x = 1)$
        \item ME at $x = 5$ (MER)
        \item ME at $x = 1$ (MER)
        \item ME at $x = 3$ (AME/MEM)
        \end{itemize}
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Discrete and Marginal Effects}
    \vspace{-1.5em}
    \begin{center}
    \begin{tikzpicture}[yscale=3.5, xscale=0.9]
    \draw [thin, step=0.1, lightgray] (0,0) grid (10,1);
    \draw [thin, gray] (0,0) grid (10,1);
    \node[right] () at (10,0) {\small $x$};
    \node[above] () at (0,1) {\small $\Pr(y=1|x)$};
    \foreach \x in {0,...,10} {
        \node[below] () at (\x,0) {\footnotesize \x};
    }
    \foreach \y in {0.0,0.2,0.4,0.6,0.8,1.0} {
    \node[left] () at (0,\y) {\footnotesize \y};
    }
    % paste0("(", seq(0,10,by=0.5), ",", round(plogis(-5 + 1.5*(seq(0,10,by=0.5))),4), ")", collapse = " -- ")
    \draw [thick, black] (0,0.0067) -- (0.5,0.0141) -- (1,0.0293) -- (1.5,0.0601) -- (2,0.1192) -- (2.5,0.2227) -- (3,0.3775) -- (3.5,0.5622) -- (4,0.7311) -- (4.5,0.852) -- (5,0.9241) -- (5.5,0.9627) -- (6,0.982) -- (6.5,0.9914) -- (7,0.9959) -- (7.5,0.9981) -- (8,0.9991) -- (8.5,0.9996) -- (9,0.9998) -- (9.5,0.9999) -- (10,1);
    
    % discrete change
    % plogis(-5 + (1.5 * 1))
    % plogis(-5 + (1.5 * 5))
    \draw<2> [red] (1,0.0293) -- (5,0.0293);
    \draw<2> [red] (5,0.0293) -- (5,0.9241);
    \draw<2> [very thick, red] (1,0.0293) -- (5,0.9241);
    \node<2>[right,red] () at (5, 0.5) {$\Delta x$};
    \node<2>[above,red] () at (3, 0) {$\Delta x$};
    
    % ME at x = 1
    % dlogis(-5 + (1.5 * 1))
    \draw<3> [very thick, blue] (0,0) -- (10,0.2845);
    
    % ME at x = 5
    % dlogis(-5 + (1.5 * 5))
    \draw<4> [very thick, green] (0,0.5736) -- (10,1.2746);
    
    % ME at x = 3
    % dlogis(-5 + (1.5 * 3))
    \draw<5> [very thick, purple] (1,-0.242) -- (5,0.99);
        
    
    \end{tikzpicture}
    \end{center}
    
    \vspace{-2em}
    
    {\small
    \begin{itemize}
    \item<2-6> Discrete change: 0.9241 - 0.0293 = 0.8948
    \item<3-6> ME at $x = 1$: 0.0285
    \item<4-6> ME at $x = 5$: 0.0701
    \item<5-6> ME at $x = 3$: 0.2350
    \end{itemize}
    }
    
\end{frame}

\frame{
    \frametitle{Language of Interpretation}
    
    \small
    
    \begin{itemize}\itemsep0.5em
    \item \textbf{Discrete effect}: A change in $x$ from $a$ to $b$ results in an increase in the predicted probability that $y$ equals 1 of 0.89, which is a substantively large and statistically significant effect.
    \item \textbf{Marginal effect}: The marginal effect of $x$ on the probability that $y$ = 1 when $x$ equals $a$ is 0.07, which is a large and statistically significant effect. The predicted probability of $y$ at this point is only 0.03, however, suggesting $x$ may be substantively unimportant for cases with this value of $x$.
    \end{itemize}
}


\frame{
    \frametitle{Summary}
    \begin{itemize}\itemsep1em
    \item Many ways to summarize GLMs
        \begin{itemize}
        \item Coefficients
        \item Predicted probabilities
        \item Marginal effects
        \item Discrete effects
        \end{itemize}
    \item<2-> Graphs help interpretation considerably
    \item<3-> There's no single correct way of summarizing a complex model
    \end{itemize}
}





\subsection{Panel Regression}
\frame{\tableofcontents[currentsection]}


\frame{

	\frametitle{Analyzing panel data}
	\begin{itemize}\itemsep1em
	\item Pooled analysis
	\item Fixed or random effects panel regression
		\begin{itemize}
		\item Both OLS-like and GLM-like approaches possible
		\end{itemize}
	\item Mixed effects models
		\begin{itemize}
		\item A generalization of random effects models
		\item Also works as framework for other hierarchical data structures (e.g., cross-national datasets)
		\end{itemize}
	\end{itemize}

}


\frame{
    \frametitle{Pooled Estimator}
    \begin{itemize}\itemsep0.5em
    \item $y_{it} = \beta_0 + \beta_1 x_{it} + \dots + \epsilon_{it}$
    \item Ignores panel structure (interdependence)
    \item Ignores heterogeneity between units
    \item But, we can actually easily estimate and interpret this model!
    \item Estimation uses ``generalized estimating equations'' (GEE)
    \item Note: Also called \textit{population-averaged} model
    \end{itemize}
}

\frame{
    \frametitle{Pooled Estimator}
    \begin{itemize}\itemsep1em
    \item Continuous outcomes:\\
        $y_{it} = \beta_0 + \beta_1 x_{it} + \dots + \epsilon_{it}$
    \item Binary outcomes:\\
        $y_{it}\ast = \beta_0 + \beta_1 x_{it} + \dots + \epsilon_{it}$\\
        $y_{it} = 1$ if $y_{it}\ast > 0$, and 0 otherwise
    \item Link functions are the same in panel as in cross-sectional
        \begin{itemize}
        \item Logit
        \item Probit
        \end{itemize}
    \item Use clustered standard errors
    \end{itemize}
}





\frame{
    \frametitle{Respecting the Panel Structure}
    \begin{itemize}\itemsep1em
    \item With a panel structure, $\epsilon_{it}$ can be decomposed into two parts:
        \begin{itemize}
        \item $\upsilon_{it}$
        \item $u_i$
        \end{itemize}
    \item If we assume $u_i$ is unrelated to $X$: fixed effects
    \item If we allow a correlation: random effects
    \end{itemize}
}

\frame{
    \frametitle{Fixed Effects Estimator}
    \begin{itemize}\itemsep1em
    \item This gives us:\\
        \begin{equation}
        \begin{array}{ll}
        y_{it} & = \beta_{0} + \beta_1 x_{it} + \dots + \upsilon_{it} + u_i\\
        y_{it} & = \beta_{0i}d_{it} + \beta_1 x_{it} + \dots + \upsilon_{it}
        \end{array}
        \end{equation}
    \item Varying intercepts (one for each unit)
    \item Can generalize to other specifications (e.g., fixed period effects)
    \end{itemize}
}


\frame{
    \frametitle{Fixed Effects Estimator}
    \begin{itemize}\itemsep1em
    \item Fixed effects terms absorb all time-invariant between-unit heterogeneity
    \item Effects of time-invariant variables cannot be estimated
    \item Each unit is its own control (``within'' estimation)
    \item For GLMs, two ways to estimate this:
        \begin{itemize}
        \item Unconditional maximum likelihood, or
        \item Conditional maximum likelihood
	    \item Both are problematic
        \end{itemize}
    \end{itemize}
}

\frame{
    \frametitle{Fixed Effects Estimator}
    \begin{itemize}\itemsep1em
    \item Unconditional maximum likelihood
        \begin{itemize}
        \item From OLS: dummy variables for each unit
        \item Number of parameters to estimate increases with sample size
        \item For logit/probit: \textit{incidental parameters problem}
        \item Estimate become inconsistent
        \end{itemize}
    \item Conditional maximum likelihood
        \begin{itemize}
        \item From OLS: ``De-meaned'' data to avoid estimating unit-specific intercepts
        \item For logit: condition on $Pr(Y_i=1)$ across all $t$ periods
        \item Does not work for probit!
        \end{itemize}
    \end{itemize}
}

\frame{
    \frametitle{Conditional MLE}
    
    \small
    
    \begin{itemize}\itemsep0.5em
    \item Estimates only based on units that change in $Y$
    \item Effects of time-invariant variables are not estimable
    \item Observations with time-invariant outcome are dropped
    \item<2-> Estimation of two-wave panel using fixed-effects logistic regression is same as a pooled logistic regression where the outcome is direction of change regressed on time-differenced explanatory variables
    \end{itemize}
}

\frame{
    \frametitle{Fixed Effects Estimator}
    \begin{itemize}\itemsep1em
    \item For linear outcomes, interpret like OLS
    \item For GLMs, interpretation is difficult because of the particulars of model estimation. You can:
        \begin{itemize}
        \item In logit: predicted log-odds or log-odds marginal effects
        \item In any GLM: assume fixed effect is zero
        \end{itemize}
    \end{itemize}
}



\frame{
    \frametitle{Random Effects Estimator}
    \begin{itemize}\itemsep1em
    \item If we are willing to assume that unit-specific error term is uncorrelated with other variables
    \item Why might this not be the case?
    \item<2-> Pooled estimator also makes this assumption
    \item<2-> But that estimator ignores panel structure (non-independence)
    \end{itemize}
}


\frame{
    \frametitle{Random Effects Estimator}
    \begin{itemize}\itemsep1em
    \item Very straightforward for continuous outcomes
	    \begin{itemize}
	    \item Interpretation is just like fixed effects, basically
	    \item Focus on predicted outcomes and/or marginal effects
	    \end{itemize}
    \item Can be used with GLMs
	    \begin{itemize}
	    \item Interpretation is messy because unit-specific error terms are unobserved
	    \item Marginal effects either on latent scale or assume random effects are zero
	    \end{itemize}
	\end{itemize}
}


\frame{
    \frametitle{Random versus Fixed Effects}
    \begin{itemize}\itemsep1em
    \item Different assumptions
    \item Very different estimation strategies
        \begin{itemize}
        \item These are consequential for interpretation for non-continuous outcomes
        \end{itemize}
	\item Common practice is to estimate multiple specifications and compare
    \end{itemize}
}


\frame{
    \frametitle{Standard Errors}
    
    \small
    
    \begin{itemize}\itemsep0.5em
    \item For continuous outcomes: Works same way as OLS but typically you will cluster by unit/observation
    \item For GLMs, standard errors can be complicated
    \item For pooled model, use standard errors clustered by unit
        \begin{itemize}
        \item \texttt{vce(robust)}
        \item \texttt{vce(cluster id)}
        \end{itemize}
    \item For random effects, you may want bootstrapped standard errors
    \item Always check for robustness
    \end{itemize}
}

\frame{
    \frametitle{Intepretation: Trade-offs}
    \begin{itemize}\itemsep1em
    \item Analytic trade-off between model choice and interpretability
    \item Pooled estimates are interpretable in conventional ways, but use assumptions
        \begin{itemize}
        \item Ignores panel structure
        \item No unobserved confounding/heterogeneity
        \end{itemize}
    \item Other models are harder to estimate and interpret, but may be more ``correct,'' though:
        \begin{itemize}
        \item RE assumes heterogeneity is not confounding
        \item FE disallows effects of time-invariant variables
        \end{itemize}
    \end{itemize}
}

\frame{
    \frametitle{{\normalsize Mixed Effects/\\Hierarchical Models}}
    
    \small
    
    \begin{itemize}\itemsep0.5em
    \item We can also estimate mixed effects models for both continuous and non-continuous outcomes
    \item This can be easier analytically than other panel model specifications for non-continuous outcomes
    \item These models also make sense when you have data that have a true hierarchical structure (such as persons within times within countries)
    \item Good resource: Gelman, A. and Hill, J., 2006. \textit{Data analysis using regression and multilevel/hierarchical models}. Cambridge University Press.
	\end{itemize}

}


\frame{

\frametitle{Final thought: Weights?}

	\begin{itemize}\itemsep1em
	\item But what about weights?
	\item Weights have specialized use in regression modelling and any default regression modelling software is going to assume data come from an SRS
	\item Use specialized software if using a complex survey design
	\end{itemize}

}

\frame{}

\frame{\tableofcontents}



\appendix
\frame{}

\end{document}
